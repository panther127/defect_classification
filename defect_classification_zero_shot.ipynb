{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQ8bO-VssnV4",
        "outputId": "74894035-b720-4025-a003-95acdcfc68ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset is stored in gdrive\n",
        "Data = '/content/gdrive/My Drive/MVTec_dataset/mvtec_anomaly_detection.tar.xz'"
      ],
      "metadata": {
        "id": "uQsEhCXZuXQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import lzma\n",
        "import tarfile\n",
        "import shutil\n",
        "import contextlib\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.models import model_from_json, Model"
      ],
      "metadata": {
        "id": "IJen5Nycv0ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# since it is tar archive file --> using Lzma to extract this archive compressed file\n",
        "with contextlib.closing(lzma.LZMAFile(Data)) as xz:\n",
        "    with tarfile.open(fileobj=xz) as f:\n",
        "        f.extractall('./MVTec-AD')"
      ],
      "metadata": {
        "id": "Z3Wfmb5avzXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preparing training set --- contains only good images\n",
        "\n",
        "# Path to the MVTec-AD dataset folder\n",
        "dataset_path = './MVTec-AD/'\n",
        "\n",
        "# List of object names\n",
        "object_names = [\n",
        "    'bottle', 'cable', 'capsule', 'carpet', 'grid', 'hazelnut',\n",
        "    'leather', 'metal_nut', 'pill', 'screw', 'tile', 'toothbrush',\n",
        "    'wood', 'transistor', 'zipper'\n",
        "]\n",
        "\n",
        "output_folder = './train'\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "for object_name in object_names:\n",
        "    object_folder = os.path.join(dataset_path, object_name, 'train', 'good')\n",
        "    good_images = [f for f in os.listdir(object_folder) if f.endswith('.png')]\n",
        "\n",
        "    for image in good_images:\n",
        "        src_path = os.path.join(object_folder, image)\n",
        "        dst_path = os.path.join(output_folder, image)\n",
        "        shutil.copy(src_path, dst_path)\n",
        "\n",
        "print(\"All good images have been collected into the 'train' folder.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ic2NM28v8A7",
        "outputId": "856c9641-56ca-4d16-e80c-8ffefcb55056"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All good images have been collected into the 'train' folder.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Resizing the training images to 256 x 256 size\n",
        "train_folder = './train'\n",
        "\n",
        "# Destination folder to store resized training images\n",
        "output_folder ='./train_normal_images'\n",
        "\n",
        "\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "# Desired image size\n",
        "target_size = (256, 256)\n",
        "\n",
        "\n",
        "for filename in os.listdir(train_folder):\n",
        "    if filename.endswith('.png'):\n",
        "        image_path = os.path.join(train_folder, filename)\n",
        "        image = Image.open(image_path)\n",
        "        image = image.convert('RGB')  # Convert to RGB format\n",
        "        image = image.resize(target_size, Image.ANTIALIAS)  # Resize\n",
        "        output_path = os.path.join(output_folder, filename)\n",
        "        image.save(output_path)\n",
        "\n",
        "print(\"Images have been resized and converted to RGB format.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpSXmuku0nvX",
        "outputId": "30328a28-b4c0-458c-bb48-abdbb6c5f73f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images have been resized and converted to RGB format.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check if some image is out of shape\n",
        "for filename in os.listdir('./train_normal_images/'):\n",
        "  image_path = os.path.join('train_normal_images', filename)\n",
        "  img = Image.open(image_path)\n",
        "  if img.size != (256, 256):\n",
        "    print()\n",
        "    print( filename, \" is Out of shape!\")\n",
        "    break\n",
        "print(\"All images are of same size\")"
      ],
      "metadata": {
        "id": "d3YNU6sC6Ino",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "457b9ab7-f6d1-41e0-b7c0-432b8559c610"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All images are of same size\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing images to numpy array\n",
        "def load_and_preprocess_images(folder_path, target_size):\n",
        "    images = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.png'):\n",
        "            image_path = os.path.join(folder_path, filename)\n",
        "            img = keras.preprocessing.image.load_img(image_path, target_size=target_size)\n",
        "            img_array = keras.preprocessing.image.img_to_array(img)\n",
        "            images.append(img_array)\n",
        "    return np.array(images)"
      ],
      "metadata": {
        "id": "sPsY_jZQ4vk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# autoencoder architecture\n",
        "def build_autoencoder(input_shape, latent_dim):\n",
        "    # Encoder\n",
        "    encoder_input = keras.Input(shape=input_shape)\n",
        "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(encoder_input)\n",
        "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    encoder_output = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "    # Decoder Architecture (Modified to output (256, 256, 3))\n",
        "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(encoder_output)\n",
        "    x = layers.UpSampling2D((2, 2))(x)\n",
        "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.UpSampling2D((2, 2))(x)\n",
        "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.UpSampling2D((2, 2))(x)\n",
        "    decoder_output = layers.Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "    # Combine encoder and decoder into an autoencoder model\n",
        "    autoencoder = keras.Model(encoder_input, decoder_output)\n",
        "    return autoencoder"
      ],
      "metadata": {
        "id": "nGOeX4bh0Gg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input shape and latent dimension for the autoencoder\n",
        "input_shape = (256, 256, 3)\n",
        "latent_dim = 64\n",
        "\n",
        "\n",
        "train_normal_data = load_and_preprocess_images('./train_normal_images/',\n",
        "                                               target_size=input_shape[:2])\n",
        "\n",
        "# Build the autoencoder\n",
        "autoencoder = build_autoencoder(input_shape, latent_dim)\n",
        "autoencoder.summary()\n",
        "\n",
        "# Compile the autoencoder\n",
        "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
        "# autoencoder.compile(optimizer='adam', loss='BinaryCrossentropy')  even after Using\n",
        "\n",
        "# Ensure that all images have the same shape (256x256)\n",
        "train_normal_data = np.array([img if img.shape == input_shape else cv2.resize(img, input_shape[:2], interpolation=cv2.INTER_AREA) for img in train_normal_data])\n",
        "\n",
        "# Normalize and reshape the data if needed\n",
        "train_normal_data = train_normal_data.astype('float32') / 255.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKxb3XB540Ab",
        "outputId": "1cb15121-37b2-4b89-9cdf-16c99716d168"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 256, 256, 3)]     0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 256, 256, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 128, 128, 32)     0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 128, 128, 64)      18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 64, 64, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 64, 64, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 32, 32, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 32, 32, 128)       147584    \n",
            "                                                                 \n",
            " up_sampling2d (UpSampling2D  (None, 64, 64, 128)      0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 64, 64, 64)        73792     \n",
            "                                                                 \n",
            " up_sampling2d_1 (UpSampling  (None, 128, 128, 64)     0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 128, 128, 32)      18464     \n",
            "                                                                 \n",
            " up_sampling2d_2 (UpSampling  (None, 256, 256, 32)     0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 256, 256, 3)       867       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 333,955\n",
            "Trainable params: 333,955\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_normal_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjkpLcGK-9fZ",
        "outputId": "2bfe2e1f-5632-4361-bcf9-4501e5fa8da0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(391, 256, 256, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the autoencoder on the normal data -- Images without any anomaly\n",
        "epochs = 50\n",
        "batch_size = 32\n",
        "autoencoder.fit(train_normal_data, train_normal_data,\n",
        "                epochs=epochs,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNhJV4i2_qEb",
        "outputId": "11d56f4d-9332-489b-80d1-e2274c19d910"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "13/13 [==============================] - 123s 9s/step - loss: 0.0732\n",
            "Epoch 2/50\n",
            "13/13 [==============================] - 113s 9s/step - loss: 0.0252\n",
            "Epoch 3/50\n",
            "13/13 [==============================] - 113s 9s/step - loss: 0.0120\n",
            "Epoch 4/50\n",
            "13/13 [==============================] - 111s 9s/step - loss: 0.0077\n",
            "Epoch 5/50\n",
            "13/13 [==============================] - 112s 9s/step - loss: 0.0051\n",
            "Epoch 6/50\n",
            "13/13 [==============================] - 108s 8s/step - loss: 0.0040\n",
            "Epoch 7/50\n",
            "13/13 [==============================] - 107s 8s/step - loss: 0.0034\n",
            "Epoch 8/50\n",
            "13/13 [==============================] - 107s 8s/step - loss: 0.0036\n",
            "Epoch 9/50\n",
            "13/13 [==============================] - 111s 8s/step - loss: 0.0030\n",
            "Epoch 10/50\n",
            "13/13 [==============================] - 106s 8s/step - loss: 0.0024\n",
            "Epoch 11/50\n",
            "13/13 [==============================] - 109s 8s/step - loss: 0.0022\n",
            "Epoch 12/50\n",
            "13/13 [==============================] - 106s 8s/step - loss: 0.0020\n",
            "Epoch 13/50\n",
            "13/13 [==============================] - 106s 8s/step - loss: 0.0019\n",
            "Epoch 14/50\n",
            "13/13 [==============================] - 112s 9s/step - loss: 0.0018\n",
            "Epoch 15/50\n",
            "13/13 [==============================] - 106s 8s/step - loss: 0.0019\n",
            "Epoch 16/50\n",
            "13/13 [==============================] - 104s 8s/step - loss: 0.0019\n",
            "Epoch 17/50\n",
            "13/13 [==============================] - 105s 8s/step - loss: 0.0017\n",
            "Epoch 18/50\n",
            "13/13 [==============================] - 105s 8s/step - loss: 0.0017\n",
            "Epoch 19/50\n",
            "13/13 [==============================] - 105s 8s/step - loss: 0.0017\n",
            "Epoch 20/50\n",
            "13/13 [==============================] - 112s 9s/step - loss: 0.0016\n",
            "Epoch 21/50\n",
            "13/13 [==============================] - 105s 8s/step - loss: 0.0016\n",
            "Epoch 22/50\n",
            "13/13 [==============================] - 105s 8s/step - loss: 0.0017\n",
            "Epoch 23/50\n",
            "13/13 [==============================] - 104s 8s/step - loss: 0.0017\n",
            "Epoch 24/50\n",
            "13/13 [==============================] - 106s 8s/step - loss: 0.0024\n",
            "Epoch 25/50\n",
            "13/13 [==============================] - 104s 8s/step - loss: 0.0020\n",
            "Epoch 26/50\n",
            "13/13 [==============================] - 112s 9s/step - loss: 0.0017\n",
            "Epoch 27/50\n",
            "13/13 [==============================] - 105s 8s/step - loss: 0.0017\n",
            "Epoch 28/50\n",
            "13/13 [==============================] - 106s 8s/step - loss: 0.0018\n",
            "Epoch 29/50\n",
            "13/13 [==============================] - 105s 8s/step - loss: 0.0016\n",
            "Epoch 30/50\n",
            "13/13 [==============================] - 105s 8s/step - loss: 0.0016\n",
            "Epoch 31/50\n",
            "13/13 [==============================] - 105s 8s/step - loss: 0.0015\n",
            "Epoch 32/50\n",
            "13/13 [==============================] - 111s 8s/step - loss: 0.0015\n",
            "Epoch 33/50\n",
            "13/13 [==============================] - 104s 8s/step - loss: 0.0015\n",
            "Epoch 34/50\n",
            "13/13 [==============================] - 104s 8s/step - loss: 0.0015\n",
            "Epoch 35/50\n",
            "13/13 [==============================] - 104s 8s/step - loss: 0.0015\n",
            "Epoch 36/50\n",
            "13/13 [==============================] - 105s 8s/step - loss: 0.0015\n",
            "Epoch 37/50\n",
            "13/13 [==============================] - 109s 8s/step - loss: 0.0015\n",
            "Epoch 38/50\n",
            "13/13 [==============================] - 106s 8s/step - loss: 0.0015\n",
            "Epoch 39/50\n",
            "13/13 [==============================] - 105s 8s/step - loss: 0.0015\n",
            "Epoch 40/50\n",
            "13/13 [==============================] - 106s 8s/step - loss: 0.0015\n",
            "Epoch 41/50\n",
            "13/13 [==============================] - 104s 8s/step - loss: 0.0015\n",
            "Epoch 42/50\n",
            "13/13 [==============================] - 104s 8s/step - loss: 0.0014\n",
            "Epoch 43/50\n",
            "13/13 [==============================] - 104s 8s/step - loss: 0.0015\n",
            "Epoch 44/50\n",
            "13/13 [==============================] - 104s 8s/step - loss: 0.0016\n",
            "Epoch 45/50\n",
            "13/13 [==============================] - 104s 8s/step - loss: 0.0015\n",
            "Epoch 46/50\n",
            "13/13 [==============================] - 105s 8s/step - loss: 0.0015\n",
            "Epoch 47/50\n",
            "13/13 [==============================] - 106s 8s/step - loss: 0.0015\n",
            "Epoch 48/50\n",
            "13/13 [==============================] - 106s 8s/step - loss: 0.0014\n",
            "Epoch 49/50\n",
            "13/13 [==============================] - 104s 8s/step - loss: 0.0014\n",
            "Epoch 50/50\n",
            "13/13 [==============================] - 104s 8s/step - loss: 0.0016\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7c0ab0772530>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_json = autoencoder.to_json()\n",
        "with open(\"autoencoder_model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# Save the learned weights to an HDF5 file\n",
        "autoencoder.save_weights(\"autoencoder_weights.h5\")"
      ],
      "metadata": {
        "id": "3Rbu-uPXiUxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # Path to the MVTec-AD dataset folders\n",
        "data_folder = './MVTec-AD/'\n",
        "\n",
        "# Path to the test folder where all images will be combined (Good + anamoly)\n",
        "combined_test_folder = 'test_images'\n",
        "\n",
        "if not os.path.exists(combined_test_folder):\n",
        "    os.makedirs(combined_test_folder)\n",
        "\n",
        "\n",
        "for obj_folder in os.listdir(data_folder):\n",
        "    if obj_folder == '.ipynb_checkpoints': continue\n",
        "    obj_test_folder = os.path.join(data_folder, obj_folder, 'test')\n",
        "\n",
        "    # Combine good and defect images into the combined test folder\n",
        "    for subfolder in os.listdir(obj_test_folder):\n",
        "        subfolder_path = os.path.join(obj_test_folder, subfolder)\n",
        "        if subfolder == 'good':\n",
        "            combined_subfolder = os.path.join(combined_test_folder, 'normal')\n",
        "        else:\n",
        "            combined_subfolder = os.path.join(combined_test_folder, 'anomaly')\n",
        "\n",
        "        # Ensure the combined subfolder exists\n",
        "        os.makedirs(combined_subfolder, exist_ok=True)\n",
        "\n",
        "        # Copy images from the current subfolder to the combined subfolder\n",
        "        for filename in os.listdir(subfolder_path):\n",
        "            src_image_path = os.path.join(subfolder_path, filename)\n",
        "            dest_image_path = os.path.join(combined_subfolder, filename)\n",
        "            shutil.copy(src_image_path, dest_image_path)"
      ],
      "metadata": {
        "id": "zdMiBV9ITv-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the test data folders\n",
        "test_data_folder = './test_images/'\n",
        "anomaly_folder = os.path.join(test_data_folder, 'anomaly')\n",
        "normal_folder = os.path.join(test_data_folder, 'normal')\n",
        "\n",
        "\n",
        "target_size = (256, 256)\n",
        "\n",
        "# Load and preprocess the images from the anomaly and normal folders\n",
        "anomaly_test_data = load_and_preprocess_images(anomaly_folder, target_size)\n",
        "normal_test_data = load_and_preprocess_images(normal_folder, target_size)\n",
        "\n",
        "# Combine the anomaly and normal test data\n",
        "test_data = np.concatenate((anomaly_test_data, normal_test_data), axis=0)\n",
        "\n",
        "\n",
        "# Normalize the pixel values to the range [0, 1]\n",
        "test_data = test_data.astype('float32') / 255.0\n",
        "test_images_reconstructed = autoencoder.predict(test_data)\n",
        "reconstruction_errors = np.mean(np.square(test_data - test_images_reconstructed), axis=(1, 2, 3))\n",
        "\n",
        "\n",
        "anomaly_threshold = 0.002\n",
        "\n",
        "# Perform Zero-Anomaly Classification\n",
        "# Note :  0 for normal images and 1 for anomalous images\n",
        "\n",
        "predictions = (reconstruction_errors > anomaly_threshold).astype(int)\n",
        "\n",
        "anomaly_indices = np.where(reconstruction_errors > anomaly_threshold)[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYbpHAWpVgYY",
        "outputId": "815f2687-d9b2-4d7e-9abb-d97b76ee1fbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 7s 2s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#true labels of the test data (0 for normal, 1 for anomaly)\n",
        "true_labels = np.concatenate((np.zeros(len(normal_test_data)), np.ones(len(anomaly_test_data))))\n",
        "\n",
        "# F1-score\n",
        "f1 = f1_score(true_labels, predictions)\n",
        "\n",
        "# confusion matrix\n",
        "conf_matrix = confusion_matrix(true_labels, predictions)\n",
        "\n",
        "print(\"F1-score:\", f1)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnUgoJp7f5PJ",
        "outputId": "b3d42753-278c-419e-d6d5-4813aab8919f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-score: 0.5233644859813085\n",
            "Confusion Matrix:\n",
            "[[11 49]\n",
            " [ 2 28]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new model that extracts embeddings from the bottleneck layer\n",
        "bottleneck_layer_name = 'conv2d_6'  # Replace with the name of your bottleneck layer\n",
        "encoder_model = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer(bottleneck_layer_name).output)\n",
        "\n",
        "# Obtain the embeddings from the encoder model\n",
        "test_embeddings = encoder_model.predict(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDUDdg8T3Hb9",
        "outputId": "c8e11f58-930e-4fa5-8c8e-faa8e58ca87c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 8s 2s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_embeddings_anomalies = {}\n",
        "\n",
        "for i in range(len(test_embeddings)):\n",
        "  semantic_embeddings_anomalies['anomaly_type_'+str(i)] = test_embeddings[i]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H35dteW7-jqk",
        "outputId": "b2827fde-fa76-4b2d-8e0f-0ccc2b7ef757"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['anomaly_type_0', 'anomaly_type_1', 'anomaly_type_2', 'anomaly_type_3', 'anomaly_type_4', 'anomaly_type_5', 'anomaly_type_6', 'anomaly_type_7', 'anomaly_type_8', 'anomaly_type_9', 'anomaly_type_10', 'anomaly_type_11', 'anomaly_type_12', 'anomaly_type_13', 'anomaly_type_14', 'anomaly_type_15', 'anomaly_type_16', 'anomaly_type_17', 'anomaly_type_18', 'anomaly_type_19', 'anomaly_type_20', 'anomaly_type_21', 'anomaly_type_22', 'anomaly_type_23', 'anomaly_type_24', 'anomaly_type_25', 'anomaly_type_26', 'anomaly_type_27', 'anomaly_type_28', 'anomaly_type_29', 'anomaly_type_30', 'anomaly_type_31', 'anomaly_type_32', 'anomaly_type_33', 'anomaly_type_34', 'anomaly_type_35', 'anomaly_type_36', 'anomaly_type_37', 'anomaly_type_38', 'anomaly_type_39', 'anomaly_type_40', 'anomaly_type_41', 'anomaly_type_42', 'anomaly_type_43', 'anomaly_type_44', 'anomaly_type_45', 'anomaly_type_46', 'anomaly_type_47', 'anomaly_type_48', 'anomaly_type_49', 'anomaly_type_50', 'anomaly_type_51', 'anomaly_type_52', 'anomaly_type_53', 'anomaly_type_54', 'anomaly_type_55', 'anomaly_type_56', 'anomaly_type_57', 'anomaly_type_58', 'anomaly_type_59', 'anomaly_type_60', 'anomaly_type_61', 'anomaly_type_62', 'anomaly_type_63', 'anomaly_type_64', 'anomaly_type_65', 'anomaly_type_66', 'anomaly_type_67', 'anomaly_type_68', 'anomaly_type_69', 'anomaly_type_70', 'anomaly_type_71', 'anomaly_type_72', 'anomaly_type_73', 'anomaly_type_74', 'anomaly_type_75', 'anomaly_type_76', 'anomaly_type_77', 'anomaly_type_78', 'anomaly_type_79', 'anomaly_type_80', 'anomaly_type_81', 'anomaly_type_82', 'anomaly_type_83', 'anomaly_type_84', 'anomaly_type_85', 'anomaly_type_86', 'anomaly_type_87', 'anomaly_type_88', 'anomaly_type_89'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity_score(embedding1, embedding2):\n",
        "    embedding1 = embedding1.flatten()\n",
        "    embedding2 = embedding2.flatten()\n",
        "\n",
        "    return np.dot(embedding1, embedding2) / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))\n",
        "\n",
        "def zero_shot_anomaly_type_classification(anomalous_embeddings, semantic_embeddings_anomalies):\n",
        "    anomaly_types = []\n",
        "    for anomaly_embedding in anomalous_embeddings:\n",
        "        best_anomaly_type = None\n",
        "        max_similarity = -1\n",
        "\n",
        "        for anomaly_type, semantic_embedding in semantic_embeddings_anomalies.items():\n",
        "            # Calculate the similarity score between the anomaly embedding and each semantic embedding\n",
        "            similarity = similarity_score(anomaly_embedding, semantic_embedding)\n",
        "            if similarity > max_similarity:\n",
        "                max_similarity = similarity\n",
        "                best_anomaly_type = anomaly_type\n",
        "\n",
        "        anomaly_types.append(best_anomaly_type)\n",
        "\n",
        "    return anomaly_types\n",
        "\n",
        "\n",
        "\n",
        "# Perform zero-shot anomaly type classification\n",
        "anomalous_embeddings = test_embeddings[anomaly_indices]\n",
        "predicted_anomaly_types = zero_shot_anomaly_type_classification(anomalous_embeddings, semantic_embeddings_anomalies)"
      ],
      "metadata": {
        "id": "Mz7P_R9ffyaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "anomaly_images = test_data[anomaly_indices]\n",
        "\n",
        "# Visualize the anomaly images along with their predicted anomaly types\n",
        "for i in range(len(anomaly_images)):\n",
        "    image = anomaly_images[i]\n",
        "    predicted_anomaly_type = predicted_anomaly_types[i]\n",
        "\n",
        "    # Plot the image\n",
        "    plt.imshow(image)\n",
        "    plt.title(f\"Predicted Anomaly Type: {predicted_anomaly_type}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "OgtN5uLzFTGF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}